{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\harpreet singh\\appdata\\local\\programs\\python\\python36-64\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# from tf.keras.models import Sequential  # This does not work!\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"labeledTrainData.csv\")\n",
    "unlabeltext = pd.read_csv(\"testData.csv\")\n",
    "\n",
    "#Inputs and Output\n",
    "X = np.array(df['review'])\n",
    "y = np.array(df['sentiment'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = .20,random_state = 42)\n",
    "\n",
    "#unlabeled dataset\n",
    "unlabeltext  = np.array(unlabeltext['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-set:   This movie is just plain dumb.<br /><br />From the casting of Ralph Meeker as Mike Hammer to the fatuous climax, the film is an exercise in wooden predictability.<br /><br />Mike Hammer is one of detective fiction's true sociopaths. Unlike Marlow and Spade, who put pieces together to solve the mystery, Hammer breaks things apart to get to the truth. This film turns Hammer into a boob by surrounding him with bad guys who are ... well, too dumb to get away with anything. One is so poorly drawn that he succumbs to a popcorn attack.<br /><br />Other parts of the movie are right out of the Three Stooges play book. Velda's dance at the barre, for instance, or the bad guy who accidentally stabs his boss in the back. And the continuity breaks are shameful: Frau Blucher is running down the centerline of the road when the camera is tight on her lower legs but she's way over the side when the camera pulls back for a wider shot. The worst break, however, precedes the popcorn attack. The bad guy stalking Hammer passes a clock seconds after our hero, except the clock shows he was seven minutes behind our guy.<br /><br />To be fair, there were some interesting camera angles and lighting, and the grand finale is so bad that it must been seen, which is the only reason that it gets two points out of 10.\n",
      "                                                                      \n",
      "Train-tokenized-set:   [11, 17, 6, 40, 1041, 989, 7, 7, 36, 1, 973, 4, 3168, 14, 1946, 4225, 5, 1, 1326, 1, 19, 6, 32, 3453, 8, 1637, 8654, 7, 7, 1946, 4225, 6, 28, 4, 1252, 280, 1021, 2, 8373, 34, 273, 1323, 291, 5, 3318, 1, 732, 4225, 2027, 180, 969, 5, 76, 5, 1, 879, 11, 19, 502, 4225, 80, 3, 31, 3394, 87, 16, 75, 491, 34, 23, 70, 96, 989, 5, 76, 242, 16, 232, 28, 6, 35, 859, 1307, 12, 27, 5, 3, 3939, 1271, 7, 7, 82, 528, 4, 1, 17, 23, 205, 43, 4, 1, 288, 4656, 294, 271, 833, 30, 1, 15, 1821, 39, 1, 75, 229, 34, 2503, 8530, 24, 1422, 8, 1, 142, 2, 1, 2382, 2027, 23, 7849, 6, 617, 177, 1, 4, 1, 1314, 51, 1, 367, 6, 2694, 20, 38, 2368, 2976, 18, 437, 93, 117, 1, 496, 51, 1, 367, 2642, 142, 15, 3, 7065, 321, 1, 246, 986, 187, 1, 3939, 1271, 1, 75, 229, 6258, 4225, 4087, 3, 5493, 1571, 100, 261, 628, 546, 1, 5493, 284, 27, 13, 1545, 230, 493, 261, 229, 7, 7, 5, 26, 1250, 47, 68, 46, 218, 367, 2441, 2, 1518, 2, 1, 1754, 1958, 6, 35, 75, 12, 9, 212, 74, 107, 60, 6, 1, 61, 279, 12, 9, 211, 104, 754, 43, 4, 155]\n"
     ]
    }
   ],
   "source": [
    "#cleaning data\n",
    "#Tokenzier\n",
    "num_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(X)\n",
    "x_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "x_test_tokens  = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "#print(tokenizer.word_index)\n",
    "print(\"Train-set:  \", (X_train[0]))\n",
    "print(\"                                                                      \")\n",
    "print(\"Train-tokenized-set:  \", (x_train_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of tokens in a sequence is:   223.7972\n",
      "The maximum number of tokens in a sequence is:   2193\n",
      "The max number of tokens we will allow is set to the average plus 2 sd   551\n",
      "This covers about 95% of the data-set:   0.94484\n"
     ]
    }
   ],
   "source": [
    "#Padding and Truncating DataÂ¶\n",
    "#The Recurrent Neural Network can take sequences of arbitrary length as input\n",
    "\n",
    "#First we count the number of tokens in all the sequences in the data-set.\n",
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "print(\"The average number of tokens in a sequence is:  \", (np.mean(num_tokens)))\n",
    "print(\"The maximum number of tokens in a sequence is:  \", (np.max(num_tokens)))\n",
    "\n",
    "#The max number of tokens we will allow is set to the average plus 2 standard deviations.\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "print(\"The max number of tokens we will allow is set to the average plus 2 sd  \", (max_tokens))\n",
    "print(\"This covers about 95% of the data-set:  \", (np.sum(num_tokens < max_tokens) / len(num_tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train-set is transformed into one big matrix of integers (tokens) (20000, 551)\n",
      "The test -set is transformed into one big matrix of integers (tokens) (5000, 551)\n",
      "                                                                      \n",
      "Tokenized training data [  11   17    6   40 1041  989    7    7   36    1  973    4 3168   14\n",
      " 1946 4225    5    1 1326    1   19    6   32 3453    8 1637 8654    7\n",
      "    7 1946 4225    6   28    4 1252  280 1021    2 8373   34  273 1323\n",
      "  291    5 3318    1  732 4225 2027  180  969    5   76    5    1  879\n",
      "   11   19  502 4225   80    3   31 3394   87   16   75  491   34   23\n",
      "   70   96  989    5   76  242   16  232   28    6   35  859 1307   12\n",
      "   27    5    3 3939 1271    7    7   82  528    4    1   17   23  205\n",
      "   43    4    1  288 4656  294  271  833   30    1   15 1821   39    1\n",
      "   75  229   34 2503 8530   24 1422    8    1  142    2    1 2382 2027\n",
      "   23 7849    6  617  177    1    4    1 1314   51    1  367    6 2694\n",
      "   20   38 2368 2976   18  437   93  117    1  496   51    1  367 2642\n",
      "  142   15    3 7065  321    1  246  986  187    1 3939 1271    1   75\n",
      "  229 6258 4225 4087    3 5493 1571  100  261  628  546    1 5493  284\n",
      "   27   13 1545  230  493  261  229    7    7    5   26 1250   47   68\n",
      "   46  218  367 2441    2 1518    2    1 1754 1958    6   35   75   12\n",
      "    9  212   74  107   60    6    1   61  279   12    9  211  104  754\n",
      "   43    4  155]\n",
      "                                                                      \n",
      "Padded    training data [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0   11   17    6   40 1041  989    7    7   36    1  973    4\n",
      " 3168   14 1946 4225    5    1 1326    1   19    6   32 3453    8 1637\n",
      " 8654    7    7 1946 4225    6   28    4 1252  280 1021    2 8373   34\n",
      "  273 1323  291    5 3318    1  732 4225 2027  180  969    5   76    5\n",
      "    1  879   11   19  502 4225   80    3   31 3394   87   16   75  491\n",
      "   34   23   70   96  989    5   76  242   16  232   28    6   35  859\n",
      " 1307   12   27    5    3 3939 1271    7    7   82  528    4    1   17\n",
      "   23  205   43    4    1  288 4656  294  271  833   30    1   15 1821\n",
      "   39    1   75  229   34 2503 8530   24 1422    8    1  142    2    1\n",
      " 2382 2027   23 7849    6  617  177    1    4    1 1314   51    1  367\n",
      "    6 2694   20   38 2368 2976   18  437   93  117    1  496   51    1\n",
      "  367 2642  142   15    3 7065  321    1  246  986  187    1 3939 1271\n",
      "    1   75  229 6258 4225 4087    3 5493 1571  100  261  628  546    1\n",
      " 5493  284   27   13 1545  230  493  261  229    7    7    5   26 1250\n",
      "   47   68   46  218  367 2441    2 1518    2    1 1754 1958    6   35\n",
      "   75   12    9  212   74  107   60    6    1   61  279   12    9  211\n",
      "  104  754   43    4  155]\n"
     ]
    }
   ],
   "source": [
    "#padding or truncating the sequences that have a different length, \n",
    "#we need to determine if we want to do this padding or truncating 'pre' or 'post'\n",
    "pad = 'pre'\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,padding=pad, truncating=pad)\n",
    "x_test_pad  = pad_sequences(x_test_tokens,  maxlen=max_tokens,padding=pad, truncating=pad)\n",
    "\n",
    "#We have now transformed the data into one big matrix of integers (tokens) with this shape:\n",
    "print(\"The train-set is transformed into one big matrix of integers (tokens)\", (x_train_pad.shape))\n",
    "print(\"The test -set is transformed into one big matrix of integers (tokens)\", (x_test_pad.shape))\n",
    "\n",
    "#Padding result\n",
    "print(\"                                                                      \")\n",
    "print(\"Tokenized training data\", (np.array(x_train_tokens[0])))\n",
    "print(\"                                                                      \")\n",
    "print(\"Padded    training data\", (x_train_pad[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-set:   This movie is just plain dumb.<br /><br />From the casting of Ralph Meeker as Mike Hammer to the fatuous climax, the film is an exercise in wooden predictability.<br /><br />Mike Hammer is one of detective fiction's true sociopaths. Unlike Marlow and Spade, who put pieces together to solve the mystery, Hammer breaks things apart to get to the truth. This film turns Hammer into a boob by surrounding him with bad guys who are ... well, too dumb to get away with anything. One is so poorly drawn that he succumbs to a popcorn attack.<br /><br />Other parts of the movie are right out of the Three Stooges play book. Velda's dance at the barre, for instance, or the bad guy who accidentally stabs his boss in the back. And the continuity breaks are shameful: Frau Blucher is running down the centerline of the road when the camera is tight on her lower legs but she's way over the side when the camera pulls back for a wider shot. The worst break, however, precedes the popcorn attack. The bad guy stalking Hammer passes a clock seconds after our hero, except the clock shows he was seven minutes behind our guy.<br /><br />To be fair, there were some interesting camera angles and lighting, and the grand finale is so bad that it must been seen, which is the only reason that it gets two points out of 10.\n",
      "                                                                      \n",
      "Tokenized text converted back to original:  this movie is just plain dumb br br from the casting of ralph as mike hammer to the climax the film is an exercise in wooden predictability br br mike hammer is one of detective true unlike and spade who put pieces together to solve the mystery hammer breaks things apart to get to the truth this film turns hammer into a by surrounding him with bad guys who are well too dumb to get away with anything one is so poorly drawn that he to a popcorn attack br br other parts of the movie are right out of the three stooges play book dance at the for instance or the bad guy who accidentally stabs his boss in the back and the continuity breaks are shameful is running down the of the road when the camera is tight on her lower legs but she's way over the side when the camera pulls back for a wider shot the worst break however the popcorn attack the bad guy stalking hammer passes a clock seconds after our hero except the clock shows he was seven minutes behind our guy br br to be fair there were some interesting camera angles and lighting and the grand finale is so bad that it must been seen which is the only reason that it gets two points out of 10\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer Inverse Map: Converting tokenized back to original text.\n",
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
    "\n",
    "#Helper-function for converting a list of tokens back to a string of words.\n",
    "def tokens_to_string(tokens):\n",
    "\n",
    "    # Map from tokens back to words.\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "\n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "\n",
    "print(\"Train-set:  \", (X_train[0]))\n",
    "print(\"                                                                      \")\n",
    "print(\"Tokenized text converted back to original: \", (tokens_to_string(x_train_tokens[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Recurrent Neural Network Model\n",
    "\n",
    "model = Sequential()\n",
    "embedding_size = 10\n",
    "\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='layer_embedding'))\n",
    "model.add(GRU(units=16, return_sequences=True))\n",
    "model.add(GRU(units=8, return_sequences=True))\n",
    "model.add(GRU(units=4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "optimizer = Adam(lr=1e-3)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 551, 10)           100000    \n",
      "_________________________________________________________________\n",
      "gru_13 (GRU)                 (None, None, 16)          1296      \n",
      "_________________________________________________________________\n",
      "gru_14 (GRU)                 (None, None, 8)           600       \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 102,057\n",
      "Trainable params: 102,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Compiling RNN model\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19000 samples, validate on 1000 samples\n",
      "Epoch 1/3\n",
      "19000/19000 [==============================]19000/19000 [==============================] - 346s 18ms/step - loss: 0.6885 - acc: 0.5396 - val_loss: 0.6622 - val_acc: 0.5910\n",
      "\n",
      "Epoch 2/3\n",
      "19000/19000 [==============================]19000/19000 [==============================] - 356s 19ms/step - loss: 0.5625 - acc: 0.7052 - val_loss: 0.4652 - val_acc: 0.7780\n",
      "\n",
      "Epoch 3/3\n",
      "19000/19000 [==============================]19000/19000 [==============================] - 346s 18ms/step - loss: 0.3818 - acc: 0.8346 - val_loss: 0.3911 - val_acc: 0.8400\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1cd6def8ba8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the Recurrent Neural NetworkÂ¶\n",
    "\n",
    "model.fit(x_train_pad, y_train,\n",
    "          validation_split=0.05, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================]5000/5000 [==============================] - 23s 5ms/step\n",
      "\n",
      "Accuracy: 84.32%\n"
     ]
    }
   ],
   "source": [
    "#Performance on Test-SetÂ¶\n",
    "result = model.evaluate(x_test_pad, y_test)\n",
    "print(\"Accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Mis-classified texts  156\n",
      "Index of first mis-classified text  18\n"
     ]
    }
   ],
   "source": [
    "#Predicted sentiment for the first 1000 texts in the test-set.\n",
    "y_pred = model.predict(x=x_test_pad[0:1000])\n",
    "y_pred = y_pred.T[0]\n",
    "\n",
    "#These predicted numbers fall between 0.0 and 1.0.\n",
    "#We use a cutoff / threshold and say that all values above 0.5 are taken to be 1.0\n",
    "cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in y_pred])\n",
    "\n",
    "#The true \"class\" for the first 1000 texts in the test-set are needed for comparison.\n",
    "cls_true = np.array(y_test[0:1000])\n",
    "\n",
    "#We can then get indices for all the texts that were incorrectly classified by comparing all the \"classes\" of these two arrays.\n",
    "incorrect = np.where(cls_pred != cls_true)\n",
    "incorrect = incorrect[0]\n",
    "\n",
    "#Of the 1000 texts used, how many were mis-classified?\n",
    "print(\"Number of Mis-classified texts \", (len(incorrect)))\n",
    "\n",
    "#Let us look at the first mis-classified text.\n",
    "print(\"Index of first mis-classified text \", (incorrect[0]))\n",
    "idx = incorrect[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  label  0.58853066\n",
      "True class label  0\n",
      "                                                                      \n",
      "Misclassified text  The 1930s saw a vogue for documentary films about remote corners of the world, with an emphasis on wild animals, exotic terrain and primitive people with unusual cultures. Despite the logistics of transporting a film crew to a distant and dangerous place, and then bringing 'em back alive (with the film footage), such films were often much cheaper to make than were conventional Hollywood features ... because there were no expensive sets, costumes, or high-priced movie stars.<br /><br />The most successful makers of such films (artistically and financially) were the team of Martin E. Johnson and his wife Osa, who made several documentaries (sometimes with blatantly staged events) in Africa and Asia. The Johnsons' safari films were extremely popular, inspiring several parodies ... most notably Wheeler & Woolsey's \\So This is Africa\\\", in which the very sexy Esther Muir plays a character named Mrs. Johnson-Martini (instead of Martin E. Johnson, geddit?). Although several other filmmakers were producing safari documentaries at this time, the Johnsons' films were the most popular in this genre because they relied heavily on humour. Viewed from our own more enlightened (I hope) standpoint, this is a serious flaw in the Johnsons' documentaries: there are too many scenes in which the funny little brown or yellow people are made to look complete idiots who are easily outsmarted by the clever white bwana Johnson and his wife.<br /><br />One definite asset of these movies is the presence of Osa Johnson. Ten years younger than her husband, she manages to seem young enough to be his daughter. While certainly not as attractive as the shapely blond Esther Muir, Osa Johnson was a pert brunette who gave ingratiating performances in front of the camera in all the films she co-produced with her husband.<br /><br />'Congorilla' is probably the best of the Johnsons' films. The shots of the Congo are interesting and have some historical value as evidence of what this environment looked like in 1930. The shots of the Pygmies and other natives are also interesting, although these suffer from the Johnsons' penchant to stage events in a manner that makes the natives look 'wild' and alien.<br /><br />The best (and funniest) scene in 'Congorilla' is an improvised sequence in which Osa Johnson attempts to teach a jazz dance to some Pygmy women. (The dance is the Black Bottom, no less ... the same dance which Bob Hope famously taught to Daisy and Violet Hilton, the conjoined twins.) Wearing jodhpurs, riding boots, and a pith helmet, Osa Johnson starts scat-singing while she does high steps and slaps her knees in her attempt to teach this dance to the African women. Meanwhile, they just stand there staring at her, apparently wondering what this crazy white woman is trying to accomplish. It's a very funny scene, but it has unpleasant undertones. Osa Johnson is doing a dance that was invented by black Americans: the implication seems to be that black Africans should instinctively be able to perform this dance after a brief demonstration (using natural rhythm, I guess) because it's in their blood, or something.<br /><br />I'll rate 'Congorilla' 4 points out of 10. This film says a little bit about African life in the 1930s and rather more about American cultural perceptions in that same decade.\"\n"
     ]
    }
   ],
   "source": [
    "#Predicted and true classes for the text:\n",
    "\n",
    "print(\"Predicted  label \", (y_pred[idx]))\n",
    "print(\"True class label \", (cls_true[idx]))\n",
    "print(\"                                                                      \")\n",
    "print(\"Misclassified text \", (X_test[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting our model on unlabelled text\n",
    "tokens = tokenizer.texts_to_sequences(unlabeltext)\n",
    "tokens_pad = pad_sequences(tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.948469  ],\n",
       "       [0.07643753],\n",
       "       [0.8616894 ],\n",
       "       ...,\n",
       "       [0.44489792],\n",
       "       [0.9364726 ],\n",
       "       [0.5162722 ]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tokens_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#References:\n",
    "#https://www.kaggle.com/c/word2vec-nlp-tutorial/data\n",
    "#https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/20_Natural_Language_Processing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
